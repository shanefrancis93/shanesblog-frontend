import os
import yaml
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
from enum import Enum
import json
import dotenv
import asyncio
import openai
import google.generativeai as genai
from anthropic import Anthropic

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Load environment variables from .env file
env_path = Path(__file__).parent / '.env'
if env_path.exists():
    dotenv.load_dotenv(env_path)
else:
    logger.warning(f".env file not found at {env_path}")

# Validate required environment variables
required_vars = ['OPENAI_API_KEY', 'ANTHROPIC_API_KEY', 'GEMINI_API_KEY']
missing_vars = [var for var in required_vars if not os.getenv(var)]
if missing_vars:
    logger.warning(f"Missing required environment variables: {', '.join(missing_vars)}")

class LLMType(Enum):
    CHATGPT = "chatgpt"
    CLAUDE = "claude"
    GEMINI = "gemini"

class LLMOrchestrator:
    def __init__(self):
        """Initialize the LLM orchestrator."""
        try:
            self.openai_client = openai.OpenAI()
            self.claude_client = Anthropic()
            
            # Initialize Gemini with API key
            gemini_api_key = os.getenv("GEMINI_API_KEY")
            if gemini_api_key:
                genai.configure(api_key=gemini_api_key)
                self.gemini_client = genai.GenerativeModel('gemini-pro')
            else:
                logger.warning("GEMINI_API_KEY not set - Gemini generation will be unavailable")
                self.gemini_client = None
                
        except Exception as e:
            logger.error(f"Error initializing LLM clients: {str(e)}")
            raise

    async def generate_with_chatgpt(self, prompt: str, context: str) -> str:
        """Generate content using ChatGPT"""
        try:
            system_prompt = """You are a professional blog writer with expertise in technology and AI. 
Write in a conversational, engaging tone while incorporating research citations.
When referencing research or studies:
1. Mention the authors' names and year in parentheses
2. Briefly explain the key findings or insights
3. Connect the research to the broader discussion

Example: "Recent research by Smith and Johnson (2023) found that AI systems trained on diverse datasets showed 40% less bias in decision-making tasks. This suggests that..."
"""
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"Context:\n{context}\n\nPrompt:\n{prompt}"}
            ]
            
            response = await asyncio.to_thread(
                self.openai_client.chat.completions.create,
                model="gpt-4-turbo-preview",
                messages=messages,
                temperature=0.7,
                max_tokens=2000
            )
            
            content = response.choices[0].message.content
            return f"{content}\n\n_(Generated by ChatGPT)_"
        except Exception as e:
            logger.error(f"ChatGPT error: {str(e)}")
            return f"Error with ChatGPT: {str(e)}"

    async def generate_with_claude(self, prompt: str, context: str) -> str:
        """Generate content using Claude"""
        try:
            system_prompt = """You are a professional blog writer with expertise in technology and AI. 
Write in a conversational, engaging tone while incorporating research citations.
When referencing research or studies:
1. Mention the authors' names and year in parentheses
2. Briefly explain the key findings or insights
3. Connect the research to the broader discussion

Example: "Recent research by Smith and Johnson (2023) found that AI systems trained on diverse datasets showed 40% less bias in decision-making tasks. This suggests that..."
"""
            
            response = await asyncio.to_thread(
                self.claude_client.messages.create,
                model="claude-3-opus-20240229",
                max_tokens=2000,
                system=system_prompt,
                messages=[{
                    "role": "user",
                    "content": f"Context:\n{context}\n\nPrompt:\n{prompt}"
                }],
                temperature=0.7
            )
            
            content = response.content[0].text
            return f"{content}\n\n_(Generated by Claude)_"
        except Exception as e:
            logger.error(f"Claude error: {str(e)}")
            return f"Error with Claude: {str(e)}"

    async def generate_with_gemini(self, prompt: str, context: str) -> str:
        """Generate content using Gemini"""
        try:
            # Configure Gemini with API key
            api_key = os.getenv("GEMINI_API_KEY")
            if not api_key:
                raise ValueError("GEMINI_API_KEY environment variable not set")
            
            genai.configure(api_key=api_key)
            model = genai.GenerativeModel('gemini-pro')
            
            system_prompt = """You are a professional blog writer with expertise in technology and AI. 
Write in a conversational, engaging tone while incorporating research citations.
When referencing research or studies:
1. Mention the authors' names and year in parentheses
2. Briefly explain the key findings or insights
3. Connect the research to the broader discussion

Example: "Recent research by Smith and Johnson (2023) found that AI systems trained on diverse datasets showed 40% less bias in decision-making tasks. This suggests that..."
"""
            
            prompt_text = f"{system_prompt}\n\nContext:\n{context}\n\nPrompt:\n{prompt}"
            response = await asyncio.to_thread(
                model.generate_content,
                prompt_text
            )
            
            if not response.text:
                raise ValueError("Empty response from Gemini")
                
            content = response.text
            return f"{content}\n\n_(Generated by Gemini)_"
        except Exception as e:
            logger.error(f"Gemini error: {str(e)}")
            return f"Error with Gemini: {str(e)}"

    async def generate_blog_section(self, llm_type: LLMType, prompt: str, context: str = "") -> str:
        """Generate blog section content using the specified LLM."""
        try:
            if llm_type == LLMType.CHATGPT:
                return await self.generate_with_chatgpt(prompt, context)
            elif llm_type == LLMType.CLAUDE:
                return await self.generate_with_claude(prompt, context)
            elif llm_type == LLMType.GEMINI:
                return await self.generate_with_gemini(prompt, context)
            else:
                raise ValueError(f"Unsupported LLM type: {llm_type}")
        except Exception as e:
            logger.error(f"Error generating content with {llm_type}: {str(e)}")
            raise

    async def get_context(self, query: str, dataset_ids: List[str] = None) -> str:
        """Get relevant context from RAGFlow."""
        # For now, return empty context as RAGFlow integration is pending
        return ""

class BlogGenerator:
    def __init__(self):
        """Initialize the blog generator with configuration."""
        self.orchestrator = LLMOrchestrator()
        self.output_dir = Path(__file__).parent / 'processed_text'
        self.output_dir.mkdir(exist_ok=True)

    def _cleanup_old_versions(self, base_name: str, keep_latest: int = 3):
        """Cleanup old versions of generated files, keeping the specified number of latest versions."""
        pattern = f"{base_name}_*.md"
        files = list(self.output_dir.glob(pattern))
        # Sort by modification time, newest first
        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        
        # Keep the specified number of latest files
        for old_file in files[keep_latest:]:
            try:
                old_file.unlink()
                logger.info(f"Cleaned up old file: {old_file}")
            except Exception as e:
                logger.error(f"Error cleaning up {old_file}: {e}")

    def _get_latest_version(self, base_name: str) -> Optional[Path]:
        """Get the latest version of a generated file."""
        pattern = f"{base_name}_*.md"
        files = list(self.output_dir.glob(pattern))
        if not files:
            return None
        # Sort by modification time, newest first
        files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
        return files[0]

    def _get_llm_type(self, model_name: str) -> LLMType:
        """Map model name to LLMType."""
        model_map = {
            "GPT-4": LLMType.CHATGPT,
            "Claude-3": LLMType.CLAUDE,
            "Gemini": LLMType.GEMINI
        }
        return model_map.get(model_name, LLMType.CHATGPT)

    async def generate_section(self, section_name: str, prompt: str, model_name: str, dataset_ids: List[str]) -> str:
        """Generate content for a specific section using the assigned AI model."""
        llm_type = self._get_llm_type(model_name)
        
        try:
            # Get relevant context from RAGFlow
            context = await self.orchestrator.get_context(prompt, dataset_ids)
            
            # Generate content with context
            content = await self.orchestrator.generate_blog_section(
                llm_type=llm_type,
                prompt=prompt,
                context=context
            )
            
            return content
            
        except Exception as e:
            logger.error(f"Error generating section {section_name}: {str(e)}")
            return f"Error generating section {section_name}: {str(e)}"

    async def generate_blog_post(self, template_path: str):
        """Generate content for all sections in the blog post."""
        try:
            template_path = Path(template_path)
            base_name = template_path.stem
            
            # Load and parse template file
            with open(template_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            # Parse frontmatter
            if content.startswith('---'):
                _, fm, _ = content.split('---', 2)
                metadata = yaml.safe_load(fm)
            else:
                metadata = {"title": "", "date": datetime.now().isoformat()}
            
            # Generate content for each section
            generated_content = {}
            for section in metadata.get('sections', []):
                section_name = section.get('name', 'Untitled')
                logger.info(f"Generating section: {section_name}")
                
                content = await self.generate_section(
                    section_name,
                    section.get('prompt', ''),
                    section.get('model', 'GPT-4'),
                    dataset_ids=[]  # We'll implement this with RAGFlow later
                )
                generated_content[section_name] = content
            
            # Generate output filename with timestamp
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_path = self.output_dir / f"{base_name}_{timestamp}.md"
            
            # Create latest symlink
            latest_link = self.output_dir / f"{base_name}_latest.md"
            
            # Prepare output content
            output_content = ['---']
            metadata['generated_at'] = datetime.now().isoformat()
            metadata['template_source'] = str(template_path)
            metadata['content'] = generated_content
            output_content.append(yaml.dump(metadata, default_flow_style=False))
            output_content.append('---\n')
            
            # Add generated sections
            for section_name, section_content in generated_content.items():
                output_content.append(f'## {section_name}\n')
                output_content.append(f'{section_content}\n')
            
            # Save to file
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write('\n'.join(output_content))
            
            # Update latest symlink
            if latest_link.exists():
                latest_link.unlink()
            latest_link.symlink_to(output_path)
            
            # Cleanup old versions
            self._cleanup_old_versions(base_name)
            
            logger.info(f"Generated blog post saved to: {output_path}")
            logger.info(f"Latest version symlinked to: {latest_link}")
            return output_path
            
        except Exception as e:
            logger.error(f"Error generating blog post: {str(e)}")
            return None

async def main():
    """Main entry point for blog generation."""
    if len(sys.argv) < 2:
        print("Please provide the path to a markdown file")
        return
    
    markdown_path = sys.argv[1]
    generator = BlogGenerator()
    await generator.generate_blog_post(markdown_path)

if __name__ == "__main__":
    import sys
    asyncio.run(main())
